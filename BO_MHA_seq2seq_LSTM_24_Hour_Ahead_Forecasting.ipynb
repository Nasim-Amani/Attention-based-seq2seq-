{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nasim-Amani/Attention-based-seq2seq-/blob/main/BO_MHA_seq2seq_LSTM_24_Hour_Ahead_Forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N4ikX_FfwLQ",
        "outputId": "37e6e07a-0172-432c-ad9f-db5d593b9cc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras_tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (2024.7.4)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras_tuner\n",
            "Successfully installed keras_tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8JX7IoTd2vx"
      },
      "outputs": [],
      "source": [
        "\n",
        "import keras_tuner\n",
        "from keras_tuner import RandomSearch\n",
        "from keras_tuner import HyperModel\n",
        "from keras_tuner import HyperParameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLW7VEcPhVE9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import (mean_squared_error, mean_absolute_error,\n",
        "                             r2_score, mean_absolute_percentage_error)\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import (Input, TimeDistributed, LSTM, Concatenate,\n",
        "                          Dense, Dropout, Attention)\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = pd.read_csv(\"metro_all6.csv\")\n",
        "columns_to_keep = ['datetime', 'Load',\n",
        "                   'Load_previous_hour','Load_same_hour_previous_day',\n",
        "                   'day_part_encoded',\n",
        "                   'ALLSKY_SFC_UV_INDEX',\n",
        "                   'month_sin','month_cos','day_of_week_num_sin' , 'day_of_week_num_cos']\n",
        "df = data[columns_to_keep]\n",
        "df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "df.set_index('datetime', inplace=True)\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "total_size = len(df)\n",
        "train_size = int(total_size * 0.5)\n",
        "valid_size = int(total_size * 0.2)\n",
        "test_size = total_size - train_size - valid_size\n",
        "\n",
        "train = df.iloc[:train_size]\n",
        "valid = df.iloc[train_size:train_size+valid_size]\n",
        "test = df.iloc[train_size+valid_size:]\n",
        "\n",
        "# Create scaler and fit on the training data\n",
        "scaler = MinMaxScaler()\n",
        "train_scaled = scaler.fit_transform(train[['Load',\n",
        "                   'Load_previous_hour','Load_same_hour_previous_day',\n",
        "                   'day_part_encoded',\n",
        "                   'ALLSKY_SFC_UV_INDEX',\n",
        "                   'month_sin','month_cos','day_of_week_num_sin' , 'day_of_week_num_cos']])\n",
        "\n",
        "# Apply the same scaler to the validation and test sets\n",
        "valid_scaled = scaler.transform(valid[['Load',\n",
        "                   'Load_previous_hour','Load_same_hour_previous_day',\n",
        "                   'day_part_encoded',\n",
        "                   'ALLSKY_SFC_UV_INDEX',\n",
        "                   'month_sin','month_cos','day_of_week_num_sin' , 'day_of_week_num_cos']])\n",
        "test_scaled = scaler.transform(test[['Load',\n",
        "                   'Load_previous_hour','Load_same_hour_previous_day',\n",
        "                   'day_part_encoded',\n",
        "                   'ALLSKY_SFC_UV_INDEX',\n",
        "                   'month_sin','month_cos','day_of_week_num_sin' , 'day_of_week_num_cos']])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_seq2seq_data(data, sequence_length, num_prediction_steps):\n",
        "    X_enc = []\n",
        "    X_dec = []\n",
        "    y_dec = []\n",
        "\n",
        "    for i in range(sequence_length, len(data) - num_prediction_steps):\n",
        "        # Encoder input\n",
        "        encoder_start = i - sequence_length\n",
        "        encoder_end = i\n",
        "        X_enc.append(data[encoder_start:encoder_end, :])  # Select only the first column\n",
        "\n",
        "        # Decoder input\n",
        "        decoder_start = i - sequence_length + 1\n",
        "        decoder_end = i + 1\n",
        "        X_dec.append(data[decoder_start:decoder_end, 0:1])  # Select only the first column\n",
        "\n",
        "        # Decoder output\n",
        "        dec_out_start = i + 1\n",
        "        dec_out_end = i + 1 + num_prediction_steps\n",
        "        y_dec.append(data[dec_out_start:dec_out_end, 0])  # Select only the first column\n",
        "\n",
        "    return np.array(X_enc), np.array(X_dec), np.array(y_dec)\n",
        "\n",
        "\n",
        "\n",
        "sequence_length = 24\n",
        "num_prediction = 24\n",
        "\n",
        "# Create the sequence data for train, validation, and test sets\n",
        "X_enc_train, X_dec_train, y_train = create_seq2seq_data(train_scaled, sequence_length, num_prediction)\n",
        "X_enc_valid, X_dec_valid, y_valid = create_seq2seq_data(valid_scaled, sequence_length, num_prediction)\n",
        "\n",
        "# Reshape the data to change the second dimension with the third dimension\n",
        "X_enc_train = np.transpose(X_enc_train, (0, 2, 1))\n",
        "X_dec_train = np.transpose(X_dec_train, (0, 2, 1))\n",
        "y_train = np.expand_dims(y_train, axis=2)\n",
        "y_train = np.transpose(y_train, (0, 2, 1))\n",
        "\n",
        "\n",
        "X_enc_valid = np.transpose(X_enc_valid, (0, 2, 1))\n",
        "X_dec_valid = np.transpose(X_dec_valid, (0, 2, 1))\n",
        "y_valid = np.expand_dims(y_valid, axis=2)\n",
        "y_valid = np.transpose(y_valid, (0, 2, 1))\n",
        "# Print the reshaped data shapes\n",
        "print(\"Encoder input shape (train):\", X_enc_train.shape)\n",
        "print(\"Decoder input shape (train):\", X_dec_train.shape)\n",
        "print(\"Output shape (train):\", y_train.shape)\n",
        "print(\"Encoder input shape (valid):\", X_enc_valid.shape)\n",
        "print(\"Decoder input shape (valid):\", X_dec_valid.shape)\n",
        "print(\"Output shape (valid):\", y_valid.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_8ljDSkshlI",
        "outputId": "8a7d4372-de50-4444-bc45-813a1afde5c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder input shape (train): (10859, 9, 24)\n",
            "Decoder input shape (train): (10859, 1, 24)\n",
            "Output shape (train): (10859, 1, 24)\n",
            "Encoder input shape (valid): (4315, 9, 24)\n",
            "Decoder input shape (valid): (4315, 1, 24)\n",
            "Output shape (valid): (4315, 1, 24)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-ca1f26d607db>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['datetime'] = pd.to_datetime(df['datetime'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "    # Hyperparameter search\n",
        "    num_layers_enc= hp.Choice('num_layers_enc',values=[1, 2] )\n",
        "    num_layers_dec = hp.Choice('num_layers_dec',values=[1, 2] )\n",
        "    units = hp.Int('units' , 16, 256, step=16)\n",
        "    encoder_dropout_rates = [hp.Choice('encoder_dropout_rates_' + str(i), values=[0.0, 0.1, 0.2, 0.4, 0.6]) for i in range(num_layers_enc)]\n",
        "    decoder_dropout_rates = [hp.Choice('decoder_dropout_rates_' + str(i), values=[0.0, 0.1, 0.2, 0.4, 0.6]) for i in range(num_layers_dec)]\n",
        "    num_heads =hp.Int('num_heads' , 1, 8, step=1)\n",
        "    #batch_size = hp.Choice('batch_size', values=[16, 32, 64, 128, 256])\n",
        "    learning_rate = hp.Choice('learning_rate', values=[0.0001, 0.001, 0.01, 0.1, 0.2])\n",
        "\n",
        "\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(X_enc_train.shape[1], X_enc_train.shape[2]))\n",
        "    encoder_layers = []\n",
        "    encoder_states = []\n",
        "    for i in range(num_layers_enc):\n",
        "        encoder_layer = LSTM(units, return_state=True, return_sequences=True)\n",
        "        if i == 0:\n",
        "            encoder_layer_output, encoder_layer_state_h, encoder_layer_state_c = encoder_layer(encoder_inputs)\n",
        "        else:\n",
        "            encoder_layer_output, encoder_layer_state_h, encoder_layer_state_c = encoder_layer(encoder_layers[-1])\n",
        "        encoder_layer_dropout = Dropout(encoder_dropout_rates[i])(encoder_layer_output)\n",
        "        encoder_layers.append(encoder_layer_dropout)\n",
        "        encoder_states.append([encoder_layer_state_h, encoder_layer_state_c])\n",
        "\n",
        "    # Decoder\n",
        "    decoder_input = Input(shape=(X_dec_train.shape[1], X_dec_train.shape[2]))\n",
        "    decoder_layers = []\n",
        "    for i in range(num_layers_dec):\n",
        "        decoder_layer = LSTM(units, return_sequences=True, return_state=True)\n",
        "        if i == 0:\n",
        "            decoder_layer_output, decoder_state_h, decoder_state_c = decoder_layer(\n",
        "                decoder_input, initial_state=encoder_states[-1])\n",
        "        else:\n",
        "            decoder_layer_output, decoder_state_h, decoder_state_c = decoder_layer(\n",
        "                concatenated_input, initial_state=[decoder_state_h, decoder_state_c])\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attention_layer = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,key_dim=units//num_heads)(decoder_layer_output ,encoder_layers[-1] )\n",
        "        concatenated_input = Concatenate()([decoder_layer_output, attention_layer])\n",
        "        decoder_layers.append(concatenated_input)\n",
        "\n",
        "\n",
        "    prediction = TimeDistributed(Dense(y_train.shape[2]))(decoder_layers[-1])\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_input], prediction)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "U53sScvcCmGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_tuner import BayesianOptimization\n",
        "\n",
        "tuner = BayesianOptimization(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=50,\n",
        "    executions_per_trial=1\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# Create an instance of EarlyStopping callback\n",
        "custom_early_stopping = EarlyStopping(\n",
        "    monitor='val_loss'  # Quantity to monitor for early stopping (validation loss)\n",
        " ,   patience=2,  # Number of epochs with no improvement after which training will be stopped\n",
        ")\n",
        "\n",
        "\n",
        "# Start the hyperparameter search\n",
        "tuner.search([X_enc_train, X_dec_train], y_train, epochs=500, batch_size=64,\n",
        "             validation_data=([X_enc_valid, X_dec_valid], y_valid),\n",
        "             callbacks=[custom_early_stopping])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFt9vuKCcMXg",
        "outputId": "9d223eb2-efa2-49de-ddfe-d3d983ecb04d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 50 Complete [00h 01m 38s]\n",
            "val_loss: 0.031167559325695038\n",
            "\n",
            "Best val_loss So Far: 0.0012072663521394134\n",
            "Total elapsed time: 02h 01m 17s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a summary of the hyperparameter search results\n",
        "tuner.results_summary()\n",
        "\n",
        "# Retrieve the best model found during the hyperparameter search\n",
        "model = tuner.get_best_models()[0]"
      ],
      "metadata": {
        "id": "xlmJJp7brF1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5841b88-3b83-4428-c59b-44f7e6d8fd4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in ./untitled_project\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_loss\", direction=\"min\")\n",
            "\n",
            "Trial 39 summary\n",
            "Hyperparameters:\n",
            "num_layers_enc: 2\n",
            "num_layers_dec: 2\n",
            "units: 96\n",
            "encoder_dropout_rates_0: 0.2\n",
            "decoder_dropout_rates_0: 0.2\n",
            "num_heads: 2\n",
            "learning_rate: 0.001\n",
            "decoder_dropout_rates_1: 0.6\n",
            "encoder_dropout_rates_1: 0.2\n",
            "Score: 0.0012072663521394134\n",
            "\n",
            "Trial 02 summary\n",
            "Hyperparameters:\n",
            "num_layers_enc: 1\n",
            "num_layers_dec: 2\n",
            "units: 128\n",
            "encoder_dropout_rates_0: 0.0\n",
            "decoder_dropout_rates_0: 0.1\n",
            "num_heads: 6\n",
            "learning_rate: 0.01\n",
            "decoder_dropout_rates_1: 0.2\n",
            "Score: 0.0012395554222166538\n",
            "\n",
            "Trial 26 summary\n",
            "Hyperparameters:\n",
            "num_layers_enc: 2\n",
            "num_layers_dec: 1\n",
            "units: 160\n",
            "encoder_dropout_rates_0: 0.2\n",
            "decoder_dropout_rates_0: 0.0\n",
            "num_heads: 2\n",
            "learning_rate: 0.001\n",
            "decoder_dropout_rates_1: 0.4\n",
            "encoder_dropout_rates_1: 0.1\n",
            "Score: 0.0013087757397443056\n",
            "\n",
            "Trial 33 summary\n",
            "Hyperparameters:\n",
            "num_layers_enc: 2\n",
            "num_layers_dec: 2\n",
            "units: 48\n",
            "encoder_dropout_rates_0: 0.4\n",
            "decoder_dropout_rates_0: 0.0\n",
            "num_heads: 7\n",
            "learning_rate: 0.01\n",
            "decoder_dropout_rates_1: 0.2\n",
            "encoder_dropout_rates_1: 0.4\n",
            "Score: 0.0013135698391124606\n",
            "\n",
            "Trial 14 summary\n",
            "Hyperparameters:\n",
            "num_layers_enc: 1\n",
            "num_layers_dec: 1\n",
            "units: 208\n",
            "encoder_dropout_rates_0: 0.6\n",
            "decoder_dropout_rates_0: 0.4\n",
            "num_heads: 1\n",
            "learning_rate: 0.001\n",
            "decoder_dropout_rates_1: 0.6\n",
            "encoder_dropout_rates_1: 0.4\n",
            "Score: 0.0013395625865086913\n",
            "\n",
            "Trial 08 summary\n",
            "Hyperparameters:\n",
            "num_layers_enc: 2\n",
            "num_layers_dec: 2\n",
            "units: 160\n",
            "encoder_dropout_rates_0: 0.2\n",
            "decoder_dropout_rates_0: 0.1\n",
            "num_heads: 2\n",
            "learning_rate: 0.01\n",
            "decoder_dropout_rates_1: 0.6\n",
            "encoder_dropout_rates_1: 0.0\n",
            "Score: 0.0013933469308540225\n",
            "\n",
            "Trial 27 summary\n",
            "Hyperparameters:\n",
            "num_layers_enc: 2\n",
            "num_layers_dec: 1\n",
            "units: 32\n",
            "encoder_dropout_rates_0: 0.6\n",
            "decoder_dropout_rates_0: 0.1\n",
            "num_heads: 2\n",
            "learning_rate: 0.01\n",
            "decoder_dropout_rates_1: 0.4\n",
            "encoder_dropout_rates_1: 0.6\n",
            "Score: 0.001416619517840445\n",
            "\n",
            "Trial 23 summary\n",
            "Hyperparameters:\n",
            "num_layers_enc: 2\n",
            "num_layers_dec: 2\n",
            "units: 64\n",
            "encoder_dropout_rates_0: 0.4\n",
            "decoder_dropout_rates_0: 0.0\n",
            "num_heads: 7\n",
            "learning_rate: 0.01\n",
            "decoder_dropout_rates_1: 0.2\n",
            "encoder_dropout_rates_1: 0.4\n",
            "Score: 0.0014213535469025373\n",
            "\n",
            "Trial 41 summary\n",
            "Hyperparameters:\n",
            "num_layers_enc: 2\n",
            "num_layers_dec: 2\n",
            "units: 96\n",
            "encoder_dropout_rates_0: 0.2\n",
            "decoder_dropout_rates_0: 0.0\n",
            "num_heads: 5\n",
            "learning_rate: 0.01\n",
            "decoder_dropout_rates_1: 0.6\n",
            "encoder_dropout_rates_1: 0.0\n",
            "Score: 0.001445123227313161\n",
            "\n",
            "Trial 17 summary\n",
            "Hyperparameters:\n",
            "num_layers_enc: 1\n",
            "num_layers_dec: 2\n",
            "units: 80\n",
            "encoder_dropout_rates_0: 0.2\n",
            "decoder_dropout_rates_0: 0.2\n",
            "num_heads: 2\n",
            "learning_rate: 0.01\n",
            "decoder_dropout_rates_1: 0.4\n",
            "encoder_dropout_rates_1: 0.4\n",
            "Score: 0.0014533811481669545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "NyX7_VtHEKVD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d248f1a-0e74-41f5-99ca-8783567b67cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 9, 24)]              0         []                            \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 9, 96),              46464     ['input_1[0][0]']             \n",
            "                              (None, 96),                                                         \n",
            "                              (None, 96)]                                                         \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 9, 96)                0         ['lstm[0][0]']                \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 1, 24)]              0         []                            \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, 9, 96),              74112     ['dropout[0][0]']             \n",
            "                              (None, 96),                                                         \n",
            "                              (None, 96)]                                                         \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)               [(None, 1, 96),              46464     ['input_2[0][0]',             \n",
            "                              (None, 96),                            'lstm_1[0][1]',              \n",
            "                              (None, 96)]                            'lstm_1[0][2]']              \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 9, 96)                0         ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 1, 96)                37248     ['lstm_2[0][0]',              \n",
            " iHeadAttention)                                                     'dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 1, 192)               0         ['lstm_2[0][0]',              \n",
            "                                                                     'multi_head_attention[0][0]']\n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)               [(None, 1, 96),              110976    ['concatenate[0][0]',         \n",
            "                              (None, 96),                            'lstm_2[0][1]',              \n",
            "                              (None, 96)]                            'lstm_2[0][2]']              \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 1, 96)                37248     ['lstm_3[0][0]',              \n",
            " ltiHeadAttention)                                                   'dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 1, 192)               0         ['lstm_3[0][0]',              \n",
            " )                                                                   'multi_head_attention_1[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " time_distributed (TimeDist  (None, 1, 24)                4632      ['concatenate_1[0][0]']       \n",
            " ributed)                                                                                         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 357144 (1.36 MB)\n",
            "Trainable params: 357144 (1.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best model found during the hyperparameter search\n",
        "best_model = tuner.get_best_models()[0]\n",
        "\n",
        "# Retrieve the hyperparameter values\n",
        "config = best_model.get_config()\n",
        "num_layers = config['layers']\n",
        "units = [layer['config']['units'] for layer in config['layers'] if layer['class_name'] == 'LSTM']\n",
        "dropout_rates = [layer['config']['rate'] for layer in config['layers'] if layer['class_name'] == 'Dropout']\n",
        "\n",
        "# Print the units in each layer\n",
        "for i, unit in enumerate(units):\n",
        "    print(f\"Layer {i+1} - Units: {unit}\")\n",
        "\n",
        "# Print the dropout rates in each layer\n",
        "for i, rate in enumerate(dropout_rates):\n",
        "    print(f\"Layer {i+1} - Dropout Rate: {rate}\")"
      ],
      "metadata": {
        "id": "jKNu0diaFIHH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cb40de3-e6dd-477b-94e6-868ecf5579e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 - Units: 96\n",
            "Layer 2 - Units: 96\n",
            "Layer 3 - Units: 96\n",
            "Layer 4 - Units: 96\n",
            "Layer 1 - Dropout Rate: 0.2\n",
            "Layer 2 - Dropout Rate: 0.2\n"
          ]
        }
      ]
    }
  ]
}